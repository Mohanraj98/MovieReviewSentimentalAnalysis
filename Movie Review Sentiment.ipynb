{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip\n/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip\n/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv\n/kaggle/input/movie-review-dataset/train.tsv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing all the required packages\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom keras.utils import to_categorical\nimport nltk\nfrom  tensorflow.keras.preprocessing.text import Tokenizer\nimport nltk\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Embedding,LSTM\nfrom keras.optimizers import Adam","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read the tab-separated file\n\ntrain=pd.read_csv('../input/movie-review-dataset/train.tsv', index_col='PhraseId', sep=\"\\t\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the dependent variable sentiment\n\ntrain=train.drop(['SentenceId'], axis=1)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initialize the count vectorizer\n\ncv= CountVectorizer()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for Preprocessing the reviews by removing unwanted words\n\ndef review_clean(df):\n    \n    reviews=[]\n    lemmatizer = WordNetLemmatizer() \n    \n    for text in df['Phrase']:\n        text = re.sub(\"[^a-zA-Z]\",\" \", text)\n        token_text= word_tokenize(text.lower())\n        clean_text= [ lemmatizer.lemmatize(i)  for i in token_text]\n        reviews.append(clean_text)\n    return reviews\n","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All words in the review are turned into lower case and special characters are removed.\n\nThen the words are lemmatized.\n\n**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning and splitting of dataset\n\ny=train.Sentiment.values\nX=train.drop(['Sentiment'], axis=1)\nnltk.download('punkt')\nnltk.download('wordnet')\ntrain_tweets=review_clean(X)\n\ny_target=to_categorical(y)\nnum_classes=y_target.shape[1]\n\nX_train, X_test, y_train, y_test = train_test_split(train_tweets, y_target, test_size = 0.2, stratify=y_target)\n","execution_count":7,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding Max length and Total Words\n\nunique_words = set()\nlen_max = 0\n\nfor sent in (X_train):\n    \n    unique_words.update(sent)\n    \n    if(len_max<len(sent)):\n        len_max = len(sent)\n        \nprint(len(list(unique_words)))\nprint(len_max)","execution_count":8,"outputs":[{"output_type":"stream","text":"13736\n48\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenization the words in the reviews\n\ntokenizer = Tokenizer(num_words=len(list(unique_words)))\ntokenizer.fit_on_texts(list(X_train))\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Padding the tokenized words\n\nX_train = pad_sequences(X_train, maxlen=len_max)\nX_test = pad_sequences(X_test, maxlen=len_max)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Early Stopping\n\nearly_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_acc', patience = 2)\ncallback = [early_stopping]","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating the model\n\nmodel=Sequential()\nmodel.add(Embedding(len(list(unique_words)),300,input_length=len_max))\nmodel.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\nmodel.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\nmodel.summary()","execution_count":12,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 48, 300)           4120800   \n_________________________________________________________________\nlstm (LSTM)                  (None, 48, 128)           219648    \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 64)                49408     \n_________________________________________________________________\ndense (Dense)                (None, 100)               6500      \n_________________________________________________________________\ndropout (Dropout)            (None, 100)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 505       \n=================================================================\nTotal params: 4,396,861\nTrainable params: 4,396,861\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\n\nhistory=model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=6, batch_size=256, verbose=1, callbacks=callback)","execution_count":13,"outputs":[{"output_type":"stream","text":"Epoch 1/6\n488/488 [==============================] - 316s 647ms/step - loss: 0.9814 - accuracy: 0.6037 - val_loss: 0.8389 - val_accuracy: 0.6542\nEpoch 2/6\n488/488 [==============================] - 312s 639ms/step - loss: 0.7934 - accuracy: 0.6753 - val_loss: 0.7973 - val_accuracy: 0.6709\nEpoch 3/6\n488/488 [==============================] - 314s 644ms/step - loss: 0.7200 - accuracy: 0.7010 - val_loss: 0.8098 - val_accuracy: 0.6758\nEpoch 4/6\n488/488 [==============================] - 312s 640ms/step - loss: 0.6780 - accuracy: 0.7156 - val_loss: 0.8396 - val_accuracy: 0.6747\nEpoch 5/6\n488/488 [==============================] - 315s 646ms/step - loss: 0.6485 - accuracy: 0.7277 - val_loss: 0.8616 - val_accuracy: 0.6757\nEpoch 6/6\n488/488 [==============================] - 318s 652ms/step - loss: 0.6278 - accuracy: 0.7354 - val_loss: 0.8978 - val_accuracy: 0.6742\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting the sentiment\n\ny_pred=model.predict_classes(X_test)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred)","execution_count":16,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-88b2fc0bbe6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}